{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of TchAIcovsky.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1zd1lv6Pqse",
        "outputId": "0c6cb466-6a32-4c47-9cb0-e29442826994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install mido\n",
        "import mido\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "def list_files(startpath): #Prototype function for listing the directory\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "        subindent = ' ' * 4 * (level + 1)\n",
        "        for f in files:\n",
        "            print('{}{}'.format(subindent, f))\n",
        "            \n",
        "def load_midi(path): #Load midi file\n",
        "    mid = mido.MidiFile(path)\n",
        "    return mid\n",
        "\n",
        "def bpm_to_tempo(bpm): #Converts tempo in beats/minute to microseconds/beat\n",
        "    tempo = (60000000/bpm)\n",
        "    return tempo\n",
        "\n",
        "def tempo_to_bpm(tempo): #Converts tempo in microseconds/beat to beats/minute\n",
        "    bpm = 60000000/tempo\n",
        "    return bpm\n",
        "\n",
        "def ticks_to_seconds(ticks,tempo,tpb):\n",
        "    return tempo/1000000*ticks/tpb\n",
        "    \n",
        "print()\n",
        "\n",
        "def quantize_track(track ,ticks_per): #ticks_per denotes quantization base. 12 for 32nds, 24 for 16ths and 48 for 8ths\n",
        "    for msg in track:\n",
        "        msg.time = round(msg.time/ticks_per)*ticks_per\n",
        "    return track\n",
        "\n",
        "\n",
        "def print_verbose(mid,length):\n",
        "    for i, track in enumerate(mid.tracks):\n",
        "        print('Track {}: {}'.format(i, track.name))\n",
        "        for msg in track[0:length]:\n",
        "            print(msg)\n",
        "            \n",
        "def print_track_verbose(midi_track):\n",
        "    for msg in midi_track:\n",
        "        print(msg)\n",
        "        \n",
        "def generate_pitch_classes(): #Returns a list where the first index (0-11) corresponds to each musical note starting from A0\n",
        "    notes = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
        "    A0 = 21\n",
        "    for i in range(8):\n",
        "        for j in range(12):\n",
        "            note = A0 +i*12 + j\n",
        "            if note <= 108:\n",
        "                notes[j].append(note)\n",
        "    return notes\n",
        "\n",
        "def note_to_pitchclass(midi_note,pitch_classes, with_paus = False): #Returns a vector with a 1 for the entered note\n",
        "    pitch_class = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    for i in range(12):\n",
        "        if midi_note in pitch_classes[i]:\n",
        "            pitch_class[i] = 1\n",
        "    return pitch_class\n",
        "\n",
        "pitch_classes = generate_pitch_classes()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mido in /usr/local/lib/python3.6/dist-packages (1.2.9)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LldEFmODPqs4"
      },
      "source": [
        "classes = list(range(21,128))\n",
        "classes.insert(0,0)\n",
        "'''\n",
        "def read_txt_files(data_folder):\n",
        "    folds = []\n",
        "    for file in sorted(os.listdir(data_folder)):\n",
        "        if not file.split('.')[1] == 'txt': #Read only txt files\n",
        "            continue\n",
        "        with open(data_folder + '/' + file,'r') as f: #Read groups one at a time\n",
        "            groups = []\n",
        "            for line in f:\n",
        "                group = []\n",
        "                for event in line.split(','):\n",
        "                    event = event.split('_')\n",
        "                    note = event[0]\n",
        "                    vel = event[1]\n",
        "                    time = event[2]\n",
        "                    pitch_class = note_to_pitchclass(int(note),pitch_classes)\n",
        "                    attr = [int(note), int(vel), int(time)]\n",
        "                    attr.extend(pitch_class)\n",
        "                    group.append(attr)\n",
        "                groups.append(group)\n",
        "        folds.append(groups)\n",
        "    return folds\n",
        "'''\n",
        "def note_class(note_value,vel,classes):\n",
        "    class_vector = np.zeros((1,2*108))\n",
        "    if vel > 0:\n",
        "      class_vector[0,classes.index(note_value)*2] = 1\n",
        "    else:\n",
        "      class_vector[0,classes.index(note_value)*2+1] = 1\n",
        "    return class_vector.tolist()[0]\n",
        "\n",
        "def class_vector_to_note(vector,classes):\n",
        "    idx = np.argwhere(vector == 1)[0][0]\n",
        "    if idx % 2 == 0:\n",
        "      vel = 100\n",
        "      note = idx/2\n",
        "    else:\n",
        "      vel = 0\n",
        "      note = round(idx/2)\n",
        "    return note, vel\n",
        "\n",
        "\n",
        "def read_train_txt(train_txt):\n",
        "  path = train_txt\n",
        "  with open(path,'r') as f: #Read groups one at a time\n",
        "    note_groups = []\n",
        "    time_groups = []\n",
        "    for i,line in enumerate(f):\n",
        "      note_group = []\n",
        "      time_group = []\n",
        "      for event in line.split(','):\n",
        "        event = event.split('_')\n",
        "        note = int(event[0])\n",
        "        vel = int(event[1])\n",
        "        time = int(event[2])\n",
        "        n_class = note_class(note,vel,classes)                                             \n",
        "        note_groups.append(np.array(n_class))\n",
        "        time_groups.append(time)\n",
        "          \n",
        "    return note_groups, time_groups\n",
        "train_note, train_time = read_train_txt('train.txt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQC0a8tnPqs9"
      },
      "source": [
        "import random\n",
        "def random_training_set(data, batch_size,chunk_len, input_size, is_cuda=True, data_type = 0):\n",
        "    file_len = len(data)\n",
        "    inp = torch.Tensor(batch_size, chunk_len, input_size)\n",
        "    target = torch.Tensor(batch_size, 1, input_size)\n",
        "    for bi in range(batch_size):\n",
        "        start_index = random.randint(0, file_len - chunk_len)\n",
        "        end_index = start_index + chunk_len\n",
        "        chunk = data[start_index:end_index]\n",
        "        if data_type:\n",
        "          chunk = [[i] for i in chunk]\n",
        "        inp[bi] = torch.Tensor(chunk)\n",
        "        if data_type:\n",
        "          target[bi] = torch.Tensor([data[end_index]])\n",
        "        else:\n",
        "          target[bi] = torch.Tensor(data[end_index])\n",
        "    inp = torch.Tensor(inp)\n",
        "    target = torch.Tensor(target)\n",
        "    if is_cuda:\n",
        "        inp = inp.cuda()\n",
        "        target = target.cuda()\n",
        "    return inp, target\n",
        "#test_inp, test_target = random_training_set(train_note, 10,100,216)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08IZ4kgWPqtG"
      },
      "source": [
        "def array_to_midi(data,midi_name):\n",
        "    \n",
        "    mid = mido.MidiFile()\n",
        "    mid.ticks_per_beat = 384\n",
        "    track0 = mido.MidiTrack()\n",
        "    track1 = mido.MidiTrack()\n",
        "\n",
        "    track0.append(mido.MetaMessage('set_tempo', tempo = 500000, time = 0)) # Meta information like tempo and time signature assigned to track 0\n",
        "    track0.append(mido.MetaMessage('time_signature', numerator = 4, denominator = 4, clocks_per_click = 24, notated_32nd_notes_per_beat = 8, time = 0))\n",
        "    track0.append(mido.MetaMessage('end_of_track', time = 1))\n",
        "    mid.tracks.append(track0)\n",
        "    track1.append(mido.Message('program_change', channel = 0, program = 0, time = 0))\n",
        "    \n",
        "    for event in data: #Write notes to track 1\n",
        "      note = event[0]\n",
        "      vel = event[1]\n",
        "      time = event[2]\n",
        "      track1.append(mido.Message('note_on', channel = 0, note = note, velocity = vel, time = time))\n",
        "    track1.append(mido.MetaMessage('end_of_track', time = 1))\n",
        "    \n",
        "    mid.tracks.append(track1)\n",
        "\n",
        "    mid.save(midi_name)\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jvCtWvTUtP2",
        "outputId": "9238f525-5f9c-46ec-af47-3c4b46bf7eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_tensor = torch.Tensor(np.arange(0,10)).view(1,10,1)\n",
        "print(torch.roll(test_tensor,-1,1).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0fIBdOoPqtP"
      },
      "source": [
        "class Note_RNN(nn.Module):\n",
        "    def __init__(self, input_size, rnn_size, hidden_size, output_size, seq_len, net_type, n_layers=1):\n",
        "        super(Note_RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.net_type = net_type\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.rnn_size = rnn_size\n",
        "        self.batch_size = None\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.rnn = nn.GRU(input_size, rnn_size, n_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(rnn_size, hidden_size)\n",
        "        self.rnn2 = nn.GRU(rnn_size, rnn_size, n_layers, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_size*seq_len, output_size)\n",
        "\n",
        "    def forward(self, inp, rnn_hidden, rnn_hidden2):\n",
        "        self.batch_size = inp.shape[0]\n",
        "        out_dropout = self.dropout(inp)\n",
        "        out_rnn1, hidden = self.rnn(out_dropout, rnn_hidden)\n",
        "        out_dropout2 = self.dropout2(out_rnn1)\n",
        "        out_rnn2, hidden2 = self.rnn2(out_dropout2, rnn_hidden2)\n",
        "        out_fc1 = self.fc1(out_rnn2)\n",
        "        out_dropout3 = self.dropout3(out_fc1)\n",
        "        out_fc2 = self.fc2(out_dropout3.view(self.batch_size,seq_len*hidden_size))\n",
        "        if self.net_type == 'note':\n",
        "          output = F.softmax(out_fc2.view(self.batch_size,1,self.input_size),dim=2)\n",
        "        elif self.net_type == 'time':\n",
        "          output = F.relu(out_fc2.view(self.batch_size,1,self.input_size))\n",
        "        return output, hidden, hidden2\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.autograd.Variable(torch.zeros(self.n_layers, batch_size, self.rnn_size))\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0CB0jQ_PqtS"
      },
      "source": [
        " import datetime\n",
        "def save(model,filename):\n",
        "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
        "    torch.save(model, save_filename)\n",
        "    print('Saved as %s' % save_filename)\n",
        "    \n",
        "def train(epochs, batches_per_epoch, note_model_save_name, time_model_save_name, is_cuda=True):\n",
        "  with open('Train_log_{}.txt'.format(datetime.datetime.now()), 'w') as f:\n",
        "    print('Training for {} epochs'.format(epochs))\n",
        "    note_net.train()\n",
        "    hidden_note_rnn = note_net.init_hidden(batch_size)\n",
        "    hidden_note_rnn2 = note_net.init_hidden(batch_size)\n",
        "    hidden_time_rnn = time_net.init_hidden(batch_size)\n",
        "    hidden_time_rnn2 = time_net.init_hidden(batch_size)\n",
        "    if is_cuda:\n",
        "        hidden_note_rnn = hidden_note_rnn.cuda()\n",
        "        hidden_note_rnn2 = hidden_note_rnn2.cuda()\n",
        "        hidden_time_rnn = hidden_time_rnn.cuda()\n",
        "        hidden_time_rnn2 = hidden_time_rnn2.cuda()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss_note = 0\n",
        "        period_loss_note = 0\n",
        "        epoch_loss_time = 0\n",
        "        period_loss_time = 0\n",
        "\n",
        "        for i in range(batches_per_epoch):\n",
        "            inp, target = random_training_set(train_note, batch_size,seq_len,216)\n",
        "            if is_cuda:\n",
        "                inp = inp.cuda()\n",
        "                target = target.cuda()\n",
        "            note_net.zero_grad()\n",
        "            note_optimizer.zero_grad()\n",
        "            output_note, hidden_note_rnn, hidden_note_rnn2 = note_net(inp, hidden_note_rnn, hidden_note_rnn2)\n",
        "            loss_note = criterion_note(output_note.squeeze(), torch.argmax(target.squeeze(),dim=1))\n",
        "            loss_note.backward()\n",
        "            note_optimizer.step()\n",
        "            hidden_note_rnn = torch.autograd.Variable(hidden_note_rnn.data, requires_grad=True)\n",
        "            hidden_note_rnn2 = torch.autograd.Variable(hidden_note_rnn2.data, requires_grad=True)\n",
        "            epoch_loss_note = epoch_loss_note + loss_note.item()\n",
        "            period_loss_note = period_loss_note + loss_note.item()\n",
        "            if i % print_every == 0 and not i == 0:\n",
        "                print('Epoch {}, Batch {}/{} Note Loss: {}'.format(epoch+1,i,batches_per_epoch,period_loss_note/(batch_size*print_every)))\n",
        "                f.write('Epoch {}, Batch {}/{} Note Loss: {}\\n'.format(epoch+1,i,batches_per_epoch,period_loss_note/(batch_size*print_every)))\n",
        "                period_loss_note = 0\n",
        "        print('---------------------------------------------------------------------------------------')\n",
        "        f.write('---------------------------------------------------------------------------------------\\n')\n",
        "        for i in range(batches_per_epoch):\n",
        "            inp, target = random_training_set(train_time, batch_size,seq_len,1,data_type=1)\n",
        "            if is_cuda:\n",
        "                inp = inp.cuda()\n",
        "                target = target.cuda()\n",
        "            time_net.zero_grad()\n",
        "            time_optimizer.zero_grad()\n",
        "            output_time, hidden_time_rnn, hidden_time_rnn2 = time_net(inp, hidden_time_rnn, hidden_time_rnn2)\n",
        "            loss_time = criterion_time(output_time, target)\n",
        "            loss_time.backward()\n",
        "            time_optimizer.step()\n",
        "            hidden_time_rnn = torch.autograd.Variable(hidden_time_rnn.data, requires_grad=True)\n",
        "            hidden_time_rnn2 = torch.autograd.Variable(hidden_time_rnn2.data, requires_grad=True)\n",
        "            epoch_loss_time = epoch_loss_time + loss_time.item()\n",
        "            period_loss_time = period_loss_time + loss_time.item()\n",
        "            if i % print_every == 0 and not i == 0:\n",
        "                print('Epoch {}, Batch {}/{} Time Loss: {}'.format(epoch+1,i,batches_per_epoch,period_loss_time/(batch_size*print_every)))\n",
        "                f.write('Epoch {}, Batch {}/{} Time Loss: {}\\n'.format(epoch+1,i,batches_per_epoch,period_loss_time/(batch_size*print_every)))\n",
        "                period_loss_time = 0\n",
        "        #lr_scheduler_note.step()\n",
        "        #lr_scheduler_time.step()\n",
        "        print('Epoch {} total Note Loss: {}, total Time Loss: {}'.format(epoch+1, epoch_loss_note/(batch_size*batches_per_epoch), epoch_loss_time/(batch_size*batches_per_epoch)))\n",
        "        f.write('Epoch {} total Note Loss: {}, total Time Loss: {}\\n'.format(epoch+1, epoch_loss_note/(batch_size*batches_per_epoch), epoch_loss_time/(batch_size*batches_per_epoch)))\n",
        "    save(note_net,note_model_save_name)\n",
        "    save(time_net,time_model_save_name)\n",
        "    f.write('Saving models....')\n",
        "    f.write('Done!')\n",
        "    f.close()\n",
        "\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNo1g-O5PqtX",
        "outputId": "f3222f5b-094f-4c99-95ce-0102ec11eb5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "import time\n",
        "batch_size = 64\n",
        "epochs = 1\n",
        "hidden_size_rnn = 512\n",
        "seq_len = 100\n",
        "hidden_size = 256\n",
        "learning_rate_note = 1e-5\n",
        "learning_rate_time = 1e-5\n",
        "batches_per_epoch = 1000\n",
        "n_layers = 1\n",
        "print_every = 100\n",
        "is_cuda = True\n",
        "\n",
        "note_net = Note_RNN(\n",
        "    216,\n",
        "    hidden_size_rnn,\n",
        "    hidden_size,\n",
        "    216,\n",
        "    seq_len,\n",
        "    'note',\n",
        "    n_layers= n_layers,\n",
        ")\n",
        "time_net = Note_RNN(\n",
        "    1,\n",
        "    hidden_size_rnn,\n",
        "    hidden_size,\n",
        "    1,\n",
        "    seq_len,\n",
        "    'time',\n",
        "    n_layers= n_layers,\n",
        ")\n",
        "note_optimizer = torch.optim.Adam(note_net.parameters(), lr=learning_rate_note)\n",
        "criterion_note = nn.CrossEntropyLoss()\n",
        "time_optimizer = torch.optim.Adam(time_net.parameters(), lr=learning_rate_time)\n",
        "criterion_time = nn.MSELoss()\n",
        "#lr_scheduler_note = torch.optim.lr_scheduler.StepLR(note_optimizer,\n",
        "#                                               step_size=3,\n",
        "#                                                gamma=0.1)\n",
        "#lr_scheduler_time = torch.optim.lr_scheduler.StepLR(time_optimizer,\n",
        "#                                               step_size=3,\n",
        "#                                               gamma=0.1)\n",
        "if is_cuda:\n",
        "    note_net.cuda()\n",
        "    time_net.cuda()\n",
        "\n",
        "start = time.time()\n",
        "train(epochs,batches_per_epoch,'Note_RNN_64_512_256_1e-5_1','Time_RNN_64_512_256_1e-5_1')\n",
        "print('Total Training Time: {}'.format(time.time()-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1 epochs\n",
            "Epoch 1, Batch 100/1000 Note Loss: 0.08479624412953854\n",
            "Epoch 1, Batch 200/1000 Note Loss: 0.08385824531316757\n",
            "Epoch 1, Batch 300/1000 Note Loss: 0.08384097591042519\n",
            "Epoch 1, Batch 400/1000 Note Loss: 0.08384109929203987\n",
            "Epoch 1, Batch 500/1000 Note Loss: 0.08382478170096874\n",
            "Epoch 1, Batch 600/1000 Note Loss: 0.0838482304662466\n",
            "Epoch 1, Batch 700/1000 Note Loss: 0.08384375363588333\n",
            "Epoch 1, Batch 800/1000 Note Loss: 0.08383948557078838\n",
            "Epoch 1, Batch 900/1000 Note Loss: 0.08382445611059666\n",
            "---------------------------------------------------------------------------------------\n",
            "Epoch 1, Batch 100/1000 Time Loss: 125.99932485580445\n",
            "Epoch 1, Batch 200/1000 Time Loss: 94.69938794136047\n",
            "Epoch 1, Batch 300/1000 Time Loss: 95.12005114555359\n",
            "Epoch 1, Batch 400/1000 Time Loss: 242.1412124824524\n",
            "Epoch 1, Batch 500/1000 Time Loss: 94.33013767242431\n",
            "Epoch 1, Batch 600/1000 Time Loss: 88.79947328567505\n",
            "Epoch 1, Batch 700/1000 Time Loss: 109.28015027999878\n",
            "Epoch 1, Batch 800/1000 Time Loss: 107.54668877601624\n",
            "Epoch 1, Batch 900/1000 Time Loss: 73.96412318229676\n",
            "Epoch 1 total Note Loss: 0.08385211051255465, total Time Loss: 114.10706655883789\n",
            "Saved as Note_RNN_64_512_256_1e-5_1.pt\n",
            "Saved as Time_RNN_64_512_256_1e-5_1.pt\n",
            "Total Training Time: 209.4416651725769\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Note_RNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SLAaHb6qOnO"
      },
      "source": [
        "def out_tensor_to_inp(in_tensor):\n",
        "  out_tensor = torch.zeros((in_tensor.shape))\n",
        "  max_idxs = torch.argmax(in_tensor,dim=2).tolist()\n",
        "  for i,batch in enumerate(max_idxs):\n",
        "    for j,idx in enumerate(batch):\n",
        "      out_tensor[i,j,idx] = 1\n",
        "  return out_tensor\n",
        "\n",
        "def generate(note_net, time_net,num_notes, is_cuda=True,if_load=False):\n",
        "  if if_load:\n",
        "    note_net = torch.load(note_net)\n",
        "    time_net = torch.load(time_net)\n",
        "  out_notes = []\n",
        "  out_times = []\n",
        "  print('Generating....')\n",
        "  hidden_note_rnn = note_net.init_hidden(1)\n",
        "  hidden_note_rnn2 = note_net.init_hidden(1)\n",
        "  hidden_time_rnn = note_net.init_hidden(1)\n",
        "  hidden_time_rnn2 = time_net.init_hidden(1)\n",
        "  if is_cuda:\n",
        "    hidden_note_rnn = hidden_note_rnn.cuda()\n",
        "    hidden_note_rnn2 = hidden_note_rnn2.cuda()\n",
        "    hidden_time_rnn = hidden_time_rnn.cuda()\n",
        "    hidden_time_rnn2 = hidden_time_rnn2.cuda()\n",
        "\n",
        "  \n",
        "  note_inp, _ = random_training_set(train_note, 1,100,216)\n",
        "\n",
        "\n",
        "\n",
        "  time_inp, _ = random_training_set(train_time, 1,100,1, data_type = 1)\n",
        "\n",
        "  #Initialize hidden layer\n",
        "  if is_cuda:\n",
        "    note_inp = note_inp.cuda()\n",
        "    time_inp = time_inp.cuda()\n",
        "\n",
        "  for i in range(num_notes):\n",
        "    output_note, hidden_note_rnn, hidden_note_rnn2 = note_net(note_inp, hidden_note_rnn, hidden_note_rnn2)\n",
        "    output_time, hidden_time_rnn, hidden_time_rnn2 = time_net(time_inp, hidden_time_rnn, hidden_time_rnn2)\n",
        "    new_note = out_tensor_to_inp(output_note)\n",
        "    new_time = round(float(output_time[0,0,0]))\n",
        "\n",
        "\n",
        "    note_inp = torch.roll(note_inp,-1,1)[0,-1] = new_note\n",
        "    np_time_inp = np.roll(time_inp.cpu().numpy(),-1,axis=1)\n",
        "    print(time_inp)\n",
        "'''\n",
        "    time_inp = torch.roll(time_inp,-1,1)\n",
        "    out_notes.append(new_note)\n",
        "    out_times.append(new_time)\n",
        "\n",
        "  out_data = []\n",
        "  for i in range(len(out_notes)):\n",
        "    note = out_notes[i].squeeze()\n",
        "    time = out_times[i].cpu().squeeze().detach()\n",
        "    note, vel = class_vector_to_note(note[j].numpy(),classes)\n",
        "    event = [int(note), int(vel), int(time)]\n",
        "    out_data.append(event)\n",
        "  print('Creating Midi file')\n",
        "  array_to_midi(out_data,'New_model_test.mid') \n",
        "  print(out_data[0])\n",
        "'''\n",
        "generate(note_net,time_net,100)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}